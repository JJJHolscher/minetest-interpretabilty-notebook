{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# Jono's Minetester PPO Interpretabilty Notebook"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Policy and Image Paths"
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "# Define Paths\nSAVED_MODEL_PATH = \"ppo_treechop-v0.model\"\n\nIMAGE_FOLDER = \"screenshots/\""
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Load model, define utility functions, etc..."
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "#Import dependencies\n\nimport sys\nimport os\nimport numpy as np\nimport jax\nimport flax\nimport flax.linen as nn\nimport jax.numpy as jnp\nimport gym\nimport cv2\n\nfrom typing import Sequence, Callable\nfrom flax.linen.initializers import constant, orthogonal\nfrom PIL import Image\n\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n%matplotlib inline"
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "#Define Neural Networks\n\nclass Network(nn.Module):\n    def setup(self):\n        self.Conv_0 = nn.Conv(\n                32,\n                kernel_size=(8, 8),\n                strides=(4, 4),\n                padding=\"VALID\",\n                kernel_init=orthogonal(np.sqrt(2)),\n                bias_init=constant(0.0),\n            )\n        self.Conv_1 = nn.Conv(\n                64,\n                kernel_size=(4, 4),\n                strides=(2, 2),\n                padding=\"VALID\",\n                kernel_init=orthogonal(np.sqrt(2)),\n                bias_init=constant(0.0),\n            )\n        self.Conv_2 = nn.Conv(\n                64,\n                kernel_size=(3, 3),\n                strides=(1, 1),\n                padding=\"VALID\",\n                kernel_init=orthogonal(np.sqrt(2)),\n                bias_init=constant(0.0),\n            )\n        self.Dense_0 = nn.Dense(512, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0))\n\n        # Can this be more elegant?\n        self.layers = [self.l0, self.l1, self.l2, self.l3]\n\n    def __call__(self, x):\n        for layer in self.layers:\n            x = layer(x)\n        return x\n\n    def l0(self, x):\n        x = jnp.transpose(x, (0, 2, 3, 1))\n        x = x / (255.0)\n        return self.Conv_0(x)\n\n    def l1(self, x):\n        x = nn.relu(x)\n        return self.Conv_1(x)\n\n    def l2(self, x):\n        x = nn.relu(x)\n        return self.Conv_2(x)\n\n    def l3(self, x):\n        x = nn.relu(x)\n        x = x.reshape((x.shape[0], -1))\n        return self.Dense_0(x)\n\nclass Critic(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        return nn.Dense(1, kernel_init=orthogonal(1), bias_init=constant(0.0))(x)\n\n\nclass Actor(nn.Module):\n    action_dim: Sequence[int]\n\n    @nn.compact\n    def __call__(self, x):\n        return nn.Dense(self.action_dim, kernel_init=orthogonal(0.01), bias_init=constant(0.0))(x)"
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {
                "trusted": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "\n\u001b[3m                                       Network Summary                                        \u001b[0m\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503\u001b[1m \u001b[0m\u001b[1mpath   \u001b[0m\u001b[1m \u001b[0m\u2503\u001b[1m \u001b[0m\u001b[1mmodule \u001b[0m\u001b[1m \u001b[0m\u2503\u001b[1m \u001b[0m\u001b[1minputs             \u001b[0m\u001b[1m \u001b[0m\u2503\u001b[1m \u001b[0m\u001b[1moutputs            \u001b[0m\u001b[1m \u001b[0m\u2503\u001b[1m \u001b[0m\u001b[1mparams                    \u001b[0m\u001b[1m \u001b[0m\u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502         \u2502 Network \u2502 \u001b[2mfloat32\u001b[0m[1,4,64,64]  \u2502 \u001b[2mfloat32\u001b[0m[1,512]      \u2502                            \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Conv_0  \u2502 Conv    \u2502 \u001b[2mfloat32\u001b[0m[1,64,64,4]  \u2502 \u001b[2mfloat32\u001b[0m[1,15,15,32] \u2502 bias: \u001b[2mfloat32\u001b[0m[32]          \u2502\n\u2502         \u2502         \u2502                     \u2502                     \u2502 kernel: \u001b[2mfloat32\u001b[0m[8,8,4,32]  \u2502\n\u2502         \u2502         \u2502                     \u2502                     \u2502                            \u2502\n\u2502         \u2502         \u2502                     \u2502                     \u2502 \u001b[1m8,224 \u001b[0m\u001b[1;2m(32.9 KB)\u001b[0m            \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Conv_1  \u2502 Conv    \u2502 \u001b[2mfloat32\u001b[0m[1,15,15,32] \u2502 \u001b[2mfloat32\u001b[0m[1,6,6,64]   \u2502 bias: \u001b[2mfloat32\u001b[0m[64]          \u2502\n\u2502         \u2502         \u2502                     \u2502                     \u2502 kernel: \u001b[2mfloat32\u001b[0m[4,4,32,64] \u2502\n\u2502         \u2502         \u2502                     \u2502                     \u2502                            \u2502\n\u2502         \u2502         \u2502                     \u2502                     \u2502 \u001b[1m32,832 \u001b[0m\u001b[1;2m(131.3 KB)\u001b[0m          \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Conv_2  \u2502 Conv    \u2502 \u001b[2mfloat32\u001b[0m[1,6,6,64]   \u2502 \u001b[2mfloat32\u001b[0m[1,4,4,64]   \u2502 bias: \u001b[2mfloat32\u001b[0m[64]          \u2502\n\u2502         \u2502         \u2502                     \u2502                     \u2502 kernel: \u001b[2mfloat32\u001b[0m[3,3,64,64] \u2502\n\u2502         \u2502         \u2502                     \u2502                     \u2502                            \u2502\n\u2502         \u2502         \u2502                     \u2502                     \u2502 \u001b[1m36,928 \u001b[0m\u001b[1;2m(147.7 KB)\u001b[0m          \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Dense_0 \u2502 Dense   \u2502 \u001b[2mfloat32\u001b[0m[1,1024]     \u2502 \u001b[2mfloat32\u001b[0m[1,512]      \u2502 bias: \u001b[2mfloat32\u001b[0m[512]         \u2502\n\u2502         \u2502         \u2502                     \u2502                     \u2502 kernel: \u001b[2mfloat32\u001b[0m[1024,512]  \u2502\n\u2502         \u2502         \u2502                     \u2502                     \u2502                            \u2502\n\u2502         \u2502         \u2502                     \u2502                     \u2502 \u001b[1m524,800 \u001b[0m\u001b[1;2m(2.1 MB)\u001b[0m           \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502\u001b[1m \u001b[0m\u001b[1m       \u001b[0m\u001b[1m \u001b[0m\u2502\u001b[1m \u001b[0m\u001b[1m       \u001b[0m\u001b[1m \u001b[0m\u2502\u001b[1m \u001b[0m\u001b[1m                   \u001b[0m\u001b[1m \u001b[0m\u2502\u001b[1m \u001b[0m\u001b[1m              Total\u001b[0m\u001b[1m \u001b[0m\u2502\u001b[1m \u001b[0m\u001b[1m602,784 \u001b[0m\u001b[1;2m(2.4 MB)\u001b[0m\u001b[1m          \u001b[0m\u001b[1m \u001b[0m\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u001b[1m                                                                                              \u001b[0m\n\u001b[1m                              Total Parameters: 602,784 \u001b[0m\u001b[1;2m(2.4 MB)\u001b[0m\u001b[1m                              \u001b[0m\n\n\n"
                }
            ],
            "source": "#Load model in \n\nSEED=42\nACTION_DIMENSION = 36\nOBS_SHAPE = (1, 4, 64, 64) # (batch, timesteps, x, y)\n\nnetwork = Network()\nactor = Actor(action_dim=ACTION_DIMENSION)\ncritic = Critic()\n\nkey = jax.random.PRNGKey(SEED)\n\nkey, network_key, actor_key, critic_key = jax.random.split(key, 4)\nsample_obs = np.zeros(OBS_SHAPE,dtype=np.float32)\nprint(network.tabulate(jax.random.PRNGKey(0), sample_obs, console_kwargs={\"width\": 200}))\nnetwork_params = network.init(network_key, sample_obs)\nactor_params = actor.init(actor_key, network.apply(network_params, sample_obs)[0])\ncritic_params = critic.init(critic_key, network.apply(network_params, sample_obs)[0])\n\nwith open(SAVED_MODEL_PATH, \"rb\") as f:\n    (args, (network_params, actor_params, critic_params)) = flax.serialization.from_bytes(\n        (None, (network_params, actor_params, critic_params)), f.read()\n    )"
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "#action/id mapping\n#Punching is enabled by default\n# mouse values are (x,y) pairs\naction_mapping = {\n    0:\"mouse -25 -25\",\n    1:\"mouse -25 0\",\n    2:\"mouse -25 25\",\n    3:\"mouse 0 -25\",\n    4:\"mouse 0 0\",#null op\n    5:\"mouse 0 25\",\n    6:\"mouse 25 -25\",\n    7:\"mouse 25 0\",\n    8:\"mouse 25 25\",\n    9:\"mouse -25 -25\",\n    10:\"mouse -25 0, JUMP\",\n    11:\"mouse -25 25, JUMP\",\n    12:\"mouse 0 -25, JUMP\",\n    13:\"mouse 0 0, JUMP\",\n    14:\"mouse 0 25, JUMP\",\n    15:\"mouse 25 -25, JUMP\",\n    16:\"mouse 25 0, JUMP\",\n    17:\"mouse 25 25, JUMP\",\n    18:\"mouse -25 -25, JUMP\",\n    19:\"mouse -25 0, FORWARD\",\n    20:\"mouse -25 25, FORWARD\",\n    21:\"mouse 0 -25, FORwARD\",\n    22:\"mouse 0 0, FORWARD\",\n    23:\"mouse 0 25, FORWARD\",\n    24:\"mouse 25 -25, FORWARD\",\n    25:\"mouse 25 0, FORWARD\",\n    26:\"mouse 25 25, FORWARD\",\n    27:\"mouse -25 -25, FORWARD, JUMP\",\n    28:\"mouse -25 0, FORWARD, JUMP\",\n    29:\"mouse -25 25, FORWARD, JUMP\",\n    30:\"mouse 0 -25, FORWARD, JUMP\",\n    31:\"mouse 0 0, FORWARD, JUMP\",\n    32:\"mouse 0 25, FORWARD, JUMP\",\n    33:\"mouse 25 -25 FORWARD, JUMP\",\n    34:\"mouse 25 0, FORWARD, JUMP\",\n    35:\"mouse 25 25, FOWARD, JUMP\",\n    \n}\nforward_mask = jnp.array([0]*18+[1]*18)\njump_mask = jnp.array(([0]*9+[1]*9)*2)\nup_mask = jnp.array([1,0,0]*12)\ndown_mask = jnp.array([0,0,1]*12)\nleft_mask = jnp.array(([1]*3+[0]*6)*4)\nright_mask = jnp.array(([0]*6+[1]*3)*4)"
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "def load_images_from_folder(folder):\n    images = []\n    for filename in os.listdir(folder):\n        if filename.endswith((\".png\", \".jpg\", \".jpeg\")):  # add more image types if necessary\n            img = Image.open(os.path.join(folder, filename))\n            if img is not None:\n                img_arr = np.array(img)\n                images.append(img_arr)\n    return images"
        },
        {
            "cell_type": "code",
            "execution_count": 30,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "def plot_frames(frames):\n\n    # Create a figure with 4 subplots, one for each frame\n    fig, axs = plt.subplots(1, frames.shape[0], figsize=(12, 3))\n\n    # Loop through each frame and plot it on a separate subplot\n    for i in range(frames.shape[0]):\n        axs[i].imshow(frames[i], cmap='gray')\n        axs[i].axis('off')"
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "def transform_image(image):\n    image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n    image = cv2.resize(\n            image, (64,64), interpolation=cv2.INTER_AREA\n        )\n    return image"
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "# Load screen shots from folder\nimages = load_images_from_folder(IMAGE_FOLDER)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "def network_gradient(output_fn, target=0):\n    def partial_forward(x):\n        for layer in network.layers[target:]:\n            x = layer(x)\n        return x\n\n    def f(x):\n        hidden = network.apply(network_params, x, method=partial_forward)\n        action_logits = actor.apply(actor_params, hidden)\n        critic_values = critic.apply(critic_params, hidden)\n        params = (p for p in network_params[\"params\"]) # Dictionaries retain ordering since py3.7.\n        output = output_fn(action_logits, critic_values, hidden, params)\n        return output\n    return jax.value_and_grad(f)\n\ndef deep_dream(init_input, output_fn, target=\"input\", lr=1e3, n_iter=600, clip_low=0, clip_high=255):\n    \n    hidden,layers = network.apply(network_params,init_input)\n    if target == \"input\":\n        x = init_input\n    if target == \"l1\":\n        x = layers[0]\n    if target == \"l2\":\n        x = layers[1]\n    if target == \"l3\":\n        x = layers[2]\n    if target == \"network\":\n        x = hidden\n    \n    f = jax.jit(network_gradient(output_fn, target=target))\n    \n    for i in range(n_iter):\n        value, grad = f(x)\n        x += lr*grad\n        x = jnp.clip(x, clip_low, clip_high)\n        if i % 200 == 0:\n            print(\"Iteration:\", i, \"Value\", value)\n    \n    return x"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "#Define the yaw probablity as p(turn left)-p(turn right)\n\ndef yaw_probabilty(action_logits, critic_output, network_output, layers, orientation):\n    if orientation == \"left\":\n        x = 1\n    if orientation == \"right\":\n        x = -1\n    action_ps = jax.vmap(jax.nn.softmax)(action_logits)\n    yaw_values = jax.vmap(lambda x: jnp.dot(left_mask,x)-jnp.dot(right_mask,x))(action_ps)\n    return x*yaw_values[0]"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "def plt_raw_image(image):\n    plt.imshow(image)\n    plt.gca().axis('off')\n    plt.show()\n    \ndef plt_network_image(image, cmap='viridis'):\n    plt.imshow(transform_image(image), cmap=cmap)\n    plt.gca().axis('off')\n    plt.show()\n    \ndef plt_latent(image, channel, layer=\"network\",cmap='viridis', is_active=False):\n    a = jnp.stack([transform_image(image)]*4)[np.newaxis,...]\n    hidden, layers = network.apply(network_params,a)\n    if layer == \"l1\":\n        layer_data = layers[0][0,:,:,channel]\n    if layer == \"l2\":\n        layer_data = layers[1][0,:,:,channel]\n    if layer == \"l3\":\n        layer_data = layers[2][0,:,:,channel]\n    if layer == \"network\":\n        layer_data = hidden.reshape(16,32)\n    \n    if is_active:\n        plt.imshow(layer_data > 0, cmap=cmap)\n    else:\n        plt.imshow(layer_data, cmap=cmap)\n    plt.gca().axis('off')\n    plt.show()\n    print(\"min_val:\", jnp.min(layer_data), \"max_val:\", jnp.max(layer_data))\n    print(\"------------------------------\")\n    \ndef plt_gradient(image, channel, output_fn, layer=\"network\",cmap='viridis'):\n    network_input = jnp.stack([transform_image(image)]*4)[np.newaxis,...]\n    hidden, layers = network.apply(network_params,network_input)\n    if layer == \"input\":\n        x = network_input\n    if layer == \"l1\":\n        x = layers[0]\n    if layer == \"l2\":\n        x = layers[1]\n    if layer == \"l3\":\n        x = layers[2]\n    if layer == \"network\":\n        x = hidden\n    \n    _,gradient = jax.jit(network_gradient(output_fn, target=layer))(x)\n    \n    if layer == \"network\":\n        gradient = gradient.reshape(1,16,32,1)\n    \n        \n    plt.imshow(gradient[0,:,:,channel], cmap=cmap)\n    plt.gca().axis('off')\n    plt.show()\n    print(\"min_val:\", jnp.min(gradient[0,:,:,channel]), \"max_val:\", jnp.max(gradient[0,:,:,channel]))\n    print(\"------------------------------\")\n    \ndef print_actions(image):\n    a = jnp.stack([transform_image(image)]*4)[np.newaxis,...]\n    network_state, layers = network.apply(network_params,a)\n    action_logits = actor.apply(actor_params,network_state)\n    action_ps = jax.nn.softmax(action_logits)[0]\n    \n    yaw_value = jnp.dot(left_mask,action_ps)-jnp.dot(right_mask,action_ps)\n    pitch_value = jnp.dot(up_mask,action_ps)-jnp.dot(down_mask,action_ps)\n    jump_value = jnp.dot(jump_mask,action_ps)\n    forward_value = jnp.dot(forward_mask,action_ps)\n    \n    print(\"Yaw:\", yaw_value)\n    print(\"Pitch:\", pitch_value)\n    print(\"Forward:\", forward_value)\n    print(\"Jump:\", jump_value)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Interpreting the policy"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Deep Dreaming\nWe can use the `deep_dream` function to probe the model for high value states and inputs that trigger particular actions"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "#Deep dreaming to find high value states\n\nframes = jnp.array(np.random.rand(1,4,64,64), dtype=jnp.float32)*0.1+128\n\ndef get_critic(a,critic,c,d):\n    return critic[0][0]\nprint(\"Mean brightness before optimization:\", jnp.mean(frames))\noptimized_frames = deep_dream(frames,get_critic,n_iter=500)\nplot_frames(optimized_frames[0])"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "#Deep dreaming to find high yaw states\n\n#Effects are clearly visible when we initialize from a roughly uniform value\nframes = jnp.array(np.random.rand(1,4,64,64))*0.1+128\n\ndef f(frames):\n    last_frame = frames[-1] #The last frame is most salient\n    last_frame_squared = np.array((last_frame-np.mean(last_frame))**2)# Look at squared deviation\n    blurred_squared_last_frame = cv2.GaussianBlur(last_frame_squared,(5,5),0) #Apply smoothing\n    return blurred_squared_last_frame[np.newaxis,:,:]\n\noptimized_frames_left = deep_dream(frames,lambda a,b,c,d: yaw_probabilty(a,b,c,d,\"left\"))\noptimized_frames_right = deep_dream(frames,lambda a,b,c,d: yaw_probabilty(a,b,c,d,\"right\"))\n\namplitude_left = f(optimized_frames_left[0])\namplitude_right = f(optimized_frames_right[0])\noutput = np.concatenate([amplitude_left, amplitude_right])\n\nplot_frames(output)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Alignment between actor and critic\n\nWe notice that the actor an critic pay attention to the same set of features more than one would expect if their values were random."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "actor_matrix = actor_params[\"params\"][\"Dense_0\"][\"kernel\"]\ncritic_vector = critic_params[\"params\"][\"Dense_0\"][\"kernel\"]\nsimilarity_scores = cosine_similarity(critic_vector.reshape(1, -1), actor_matrix.T)\nplt.hist(similarity_scores.reshape(-1))\nsimilarity_scores = cosine_similarity(np.random.randn(512).reshape(1, -1), actor_matrix.T)\nplt.hist(similarity_scores.reshape(-1))\nplt.legend(['Actor/Critic cosine similarity', 'Random cosine similarity'])"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### What the model sees vs what humans see\n\nThe input image is downscaled from 600x1024x3 RGB -> 64x64 greyscale"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "image_id = 12\nimage  = images[image_id]\nprint(\"What humans see:\")\nplt_raw_image(image)\nprint(\"What the network sees:\")\nplt_network_image(image, cmap='gray')"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### The network's control system\n\nWe can see for many images with trees in them, that by mirroring the image, the sign of the yaw probablity is inverted."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "image_id = 1\nimage  = images[image_id]\nplt_network_image(image)\nprint_actions(image)\n\nimage  = images[image_id][:,::-1,:]\nplt_network_image(image)\nprint_actions(image)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Generalization to the night\n\nWe see that the control system seems to keep working with different tree types and even at night, where colors are inverted."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "image_id = 30\nimage  = images[image_id]\nplt_network_image(image)\nprint_actions(image)\n\nimage  = images[image_id][:,::-1,:]\nplt_network_image(image)\nprint_actions(image)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Drilling into Layer 1\n\nWe can plot the activation for typical images to see how the internal activations react. Values above 0 will cause a relu network to pass the value forward. For instance, we can see channel 21 acting as a left/right edge detector"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "image_id =1\nimage = images[image_id]\nprint(\"What the network sees:\")\nplt_network_image(image)\n\nprint(\"Pre ReLU activation:\")\nplt_latent(np.array(image, dtype=np.float32),21,layer=\"l1\")\nprint(\"Which ReLU's get triggered:\")\nplt_latent(np.array(image, dtype=np.float32),21,layer=\"l1\", is_active=True)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Tree detectors in layers 2 and 3\n\nWe found that channel 56 in layer 2 and channel 0 in layer 3 acted somewhat reliably as a tree detectors, even at night."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "image_id = 15\nimage = images[image_id]\nprint(\"What the network sees:\")\nplt_network_image(image)\n\nprint(\"Pre ReLU activation:\")\nplt_latent(np.array(image, dtype=np.float32),56,layer=\"l2\")\nprint(\"Which ReLU's get triggered:\")\nplt_latent(np.array(image, dtype=np.float32),56,layer=\"l2\", is_active=True)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "image_id = 15\nimage = images[image_id]\nprint(\"What the network sees:\")\nplt_network_image(image)\n\nprint(\"Pre ReLU activation:\")\nplt_latent(np.array(image, dtype=np.float32),0,layer=\"l3\")\nprint(\"Which ReLU's get triggered:\")\nplt_latent(np.array(image, dtype=np.float32),0,layer=\"l3\", is_active=True)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "image_id = 12\nimage  = images[image_id][:,::-1,:]\nprint(\"What humans see:\")\nplt_raw_image(image)\nprint(\"What the network sees:\")\nplt_network_image(image)\nprint(\"Activations:\")\nplt_latent(image,0,layer=\"l3\")\nprint(\"Gradient:\")\nplt_gradient(image,0, lambda a,b,c,d: yaw_probabilty(a,b,c,d,\"right\"),layer=\"l3\")\nprint_actions(image)"
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}